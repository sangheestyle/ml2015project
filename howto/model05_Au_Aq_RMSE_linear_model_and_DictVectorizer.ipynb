{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model05\n",
    "\n",
    "## Model\n",
    "* Linear models: LinearRegression, Ridge, Lasso, ElasticNet\n",
    "\n",
    "## Features\n",
    "* uid\n",
    "* qid\n",
    "* q_length\n",
    "* category\n",
    "* answer\n",
    "* avg_per_uid: average response time per user\n",
    "* avg_per_qid: average response time per question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with gzip.open(\"../data/train.pklz\", \"rb\") as train_file:\n",
    "    train_set = pickle.load(train_file)\n",
    "\n",
    "with gzip.open(\"../data/test.pklz\", \"rb\") as test_file:\n",
    "    test_set = pickle.load(test_file)\n",
    "\n",
    "with gzip.open(\"../data/questions.pklz\", \"rb\") as questions_file:\n",
    "    questions = pickle.load(questions_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make training set\n",
    "\n",
    "For training model, we might need to make feature and lable pair. In this case, we will use only uid, qid, and position for feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'cole', 'qid': 1, 'uid': 0, 'position': 61.0}\n",
      "['answer', 'category', 'group', 'pos_token', 'question']\n"
     ]
    }
   ],
   "source": [
    "print train_set[1]\n",
    "print questions[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "avg_time_per_user = {}\n",
    "avg_time_per_que = {}\n",
    "\n",
    "for key in train_set:\n",
    "    # We only care about positive case at this time\n",
    "    #if train_set[key]['position'] < 0:\n",
    "    #    continue\n",
    "    uid = train_set[key]['uid']\n",
    "    qid = train_set[key]['qid']\n",
    "    pos = train_set[key]['position']\n",
    "    q_length = max(questions[qid]['pos_token'].keys())\n",
    "    category = questions[qid]['category'].lower()\n",
    "    answer = questions[qid]['answer'].lower()\n",
    "    \n",
    "    # Calculate average response time per user\n",
    "    temp = 0; num = 0\n",
    "    if uid not in avg_time_per_user.keys():\n",
    "        for keysubset in train_set:\n",
    "            if train_set[keysubset]['uid'] == uid:\n",
    "                temp += train_set[keysubset]['position']\n",
    "                num += 1\n",
    "        avg_time_per_user[uid] = temp/num\n",
    "        temp=0; num = 0\n",
    "\n",
    "    # Calculate average response time per question\n",
    "    temp=0; num = 0\n",
    "    if qid not in avg_time_per_que.keys():\n",
    "        for keysubset in train_set:\n",
    "            if train_set[keysubset]['qid'] == qid:\n",
    "                temp += train_set[keysubset]['position']\n",
    "                num += 1\n",
    "        avg_time_per_que[qid] = temp/num\n",
    "        temp=0; num = 0\n",
    "    \n",
    "    feat = {\"uid\": str(uid), \"qid\": str(qid), \"q_length\": q_length, \"category\": category, \"answer\": answer, \"avg_per_uid\": avg_time_per_user[uid], \"avg_per_qid\":avg_time_per_que[qid]}\n",
    "    X.append(feat)\n",
    "    Y.append([pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28494\n",
      "28494\n",
      "{'category': 'fine arts', 'avg_per_uid': 55.708333333333336, 'avg_per_qid': 51.0, 'uid': '0', 'qid': '1', 'q_length': 77, 'answer': 'thomas cole'} [61.0]\n"
     ]
    }
   ],
   "source": [
    "print len(X)\n",
    "print len(Y)\n",
    "print X[0], Y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means that user 0 tried to solve question number 1 which has 77 tokens for question and he or she answered at 61st token.\n",
    "\n",
    "## Train model and make predictions\n",
    "\n",
    "Let's train model and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 920)\t1.0\n",
      "  (0, 1020)\t51.0\n",
      "  (0, 1021)\t55.7083333333\n",
      "  (0, 1026)\t1.0\n",
      "  (0, 1033)\t77.0\n",
      "  (0, 1034)\t1.0\n",
      "  (0, 6958)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "vec = DictVectorizer()\n",
    "X = vec.fit_transform(X)\n",
    "print X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Cross validation RMSE scores: 70.2538395585\n",
      "[ 68.39945816  73.69032503  66.82839455  63.55430443  63.46247558\n",
      "  67.52452534  68.75825003  78.33195237  80.50150679  71.4872033 ]\n",
      "Ridge Cross validation RMSE scores: 70.2511715134\n",
      "[ 66.96680032  73.97541369  66.47868624  63.23195054  63.31794102\n",
      "  67.80471778  69.02672314  79.52439608  80.78110191  71.40398441]\n",
      "Lasso Cross validation RMSE scores: 68.8414140964\n",
      "[ 65.55219108  72.41009122  65.33503897  61.77487946  62.15263106\n",
      "  66.44985357  67.68136606  77.41133691  79.46512163  70.18163101]\n",
      "ElasticNet Cross validation RMSE scores: 68.8415869663\n",
      "[ 65.55229672  72.41029208  65.33515005  61.77533796  62.1532062\n",
      "  66.44999469  67.6812933   77.41127578  79.46538065  70.18164222]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "import math\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split (X, Y)\n",
    "\n",
    "regressor = LinearRegression()\n",
    "scores = cross_val_score(regressor, X, Y, cv=10, scoring= 'mean_squared_error')\n",
    "# Flip the sign of MSE and take sqrt of that values.\n",
    "for ii in xrange(len(scores)):\n",
    "    scores[ii] = math.sqrt(-1*scores[ii])\n",
    "print 'Linear Cross validation RMSE scores:', scores.mean()\n",
    "print scores\n",
    "\n",
    "regressor = Ridge()\n",
    "scores = cross_val_score(regressor, X, Y, cv=10, scoring= 'mean_squared_error')\n",
    "# Flip the sign of MSE and take sqrt of that values.\n",
    "for ii in xrange(len(scores)):\n",
    "    scores[ii] = math.sqrt(-1*scores[ii])\n",
    "print 'Ridge Cross validation RMSE scores:', scores.mean()\n",
    "print scores\n",
    "\n",
    "regressor = Lasso()\n",
    "scores = cross_val_score(regressor, X, Y, cv=10, scoring= 'mean_squared_error')\n",
    "# Flip the sign of MSE and take sqrt of that values.\n",
    "for ii in xrange(len(scores)):\n",
    "    scores[ii] = math.sqrt(-1*scores[ii])\n",
    "print 'Lasso Cross validation RMSE scores:', scores.mean()\n",
    "print scores\n",
    "\n",
    "regressor = ElasticNet()\n",
    "scores = cross_val_score(regressor, X, Y, cv=10, scoring= 'mean_squared_error')\n",
    "# Flip the sign of MSE and take sqrt of that values.\n",
    "for ii in xrange(len(scores)):\n",
    "    scores[ii] = math.sqrt(-1*scores[ii])\n",
    "print 'ElasticNet Cross validation RMSE scores:', scores.mean()\n",
    "print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{1: 2}, {2: 3}]\n",
      "[{3: 2}, {4: 3}]\n"
     ]
    }
   ],
   "source": [
    "a = [{1: 2}, {2: 3}]\n",
    "b = [{3: 2}, {4: 3}]\n",
    "c = a + b\n",
    "print c[:len(a)]\n",
    "print c[len(a):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transform:  4749\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for key in train_set:\n",
    "    # We only care about positive case at this time\n",
    "    #if train_set[key]['position'] < 0:\n",
    "    #    continue\n",
    "    uid = train_set[key]['uid']\n",
    "    qid = train_set[key]['qid']\n",
    "    pos = train_set[key]['position']\n",
    "    q_length = max(questions[qid]['pos_token'].keys())\n",
    "    category = questions[qid]['category'].lower()\n",
    "    answer = questions[qid]['answer'].lower()\n",
    "    feat = {\"uid\": str(uid), \"qid\": str(qid), \"q_length\": q_length, \"category\": category, \"answer\": answer}\n",
    "    X_train.append(feat)\n",
    "    Y_train.append(pos)\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "for key in test_set:\n",
    "    uid = test_set[key]['uid']\n",
    "    qid = test_set[key]['qid']\n",
    "    q_length = max(questions[qid]['pos_token'].keys())\n",
    "    category = questions[qid]['category'].lower()\n",
    "    answer = questions[qid]['answer'].lower()\n",
    "    feat = {\"uid\": str(uid), \"qid\": str(qid), \"q_length\": q_length, \"category\": category, \"answer\": answer}\n",
    "    X_test.append(feat)\n",
    "    Y_test.append(key)\n",
    "\n",
    "print \"Before transform: \", len(X_test)\n",
    "X_train_length = len(X_train)\n",
    "X = vec.fit_transform(X_train + X_test)\n",
    "X_train = X[:X_train_length]\n",
    "X_test = X[X_train_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = Ridge()\n",
    "regressor.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[7, 34.876806438006554],\n",
       " [14, 55.506806653610681],\n",
       " [21, 36.62606574100618],\n",
       " [28, 24.247734632069555],\n",
       " [35, 74.999528288463722]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = regressor.predict(X_test)\n",
    "predictions = sorted([[id, predictions[index]] for index, id in enumerate(Y_test)])\n",
    "print len(predictions)\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is 4749 predictions.\n",
    "\n",
    "## Writing submission.\n",
    "\n",
    "OK, let's writing submission into guess.csv file. In the given submission form, we realized that we need to put header. So, we will insert header at the first of predictions, and then make it as a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "predictions.insert(0,[\"id\", \"position\"])\n",
    "with open('guess.csv', 'wb') as fp:\n",
    "    writer = csv.writer(fp, delimiter=',')\n",
    "    writer.writerows(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right. Let's submit!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
