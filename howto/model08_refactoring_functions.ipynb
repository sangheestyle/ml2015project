{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model08: Refactoring functions\n",
    "In this model, we do refactor all the functions we used. Especilly, functions related to feature extraction are refactored. It means it does not make any improvement in terms of prediction accuracy.\n",
    "\n",
    "Just read what has been changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Functions\n",
    "\n",
    "There have four different functions.\n",
    "\n",
    "* Data reader: Read data from file.\n",
    "* Feature functions(private): Functions which extract features are placed in here. It means that if you make a specific feature function, you can add the one into here.\n",
    "* Feature function(public): We can use only this function for feature extraction.\n",
    "* Utility functions: All the funtions except functions which are mentioned in above should be placed in here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "from os import path\n",
    "from collections import defaultdict\n",
    "from numpy import sign\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Load buzz data as a dictionary.\n",
    "You can give parameter for data so that you will get what you need only.\n",
    "\"\"\"\n",
    "def load_buzz(root='../data', data=['train', 'test', 'questions'], format='pklz'):\n",
    "    buzz_data = {}\n",
    "    for ii in data:\n",
    "        file_path = path.join(root, ii + \".\" + format)\n",
    "        with gzip.open(file_path, \"rb\") as fp:\n",
    "          buzz_data[ii] = pickle.load(fp)\n",
    "        \n",
    "    return buzz_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature functions(private)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _feat_basic(bd, group):\n",
    "    X = []\n",
    "    for item in bd[group].items():\n",
    "        qid = item[1]['qid']\n",
    "        q = bd['questions'][qid]\n",
    "        item[1]['q_length'] = max(q['pos_token'].keys())\n",
    "        item[1]['category'] = q['category'].lower()\n",
    "        item[1]['answer'] = q['answer'].lower()\n",
    "        X.append(item[1])\n",
    "        \n",
    "    return X\n",
    "        \n",
    "        \n",
    "def _feat_sign_val(data):\n",
    "    for item in data:\n",
    "        item['sign_val'] = sign(item['position'])\n",
    "\n",
    "        \n",
    "def _feat_avg_pos(data, sign_val):\n",
    "    unwanted_index = []\n",
    "    pos_uid = defaultdict(list)\n",
    "    pos_qid = defaultdict(list)\n",
    "    \n",
    "    for index, item in enumerate(data):\n",
    "        if sign_val and sign(item['position']) != sign_val:\n",
    "            unwanted_index.append(index)\n",
    "        else:\n",
    "            pos_uid[item['uid']].append(item['position'])\n",
    "            pos_qid[item['qid']].append(item['position'])\n",
    "\n",
    "    avg_pos_uid = {}\n",
    "    avg_pos_qid = {}\n",
    "\n",
    "    for key in pos_uid:\n",
    "        avg_pos_uid[key] = sum(pos_uid[key]) / len(pos_uid[key])\n",
    "\n",
    "    for key in pos_qid:\n",
    "        avg_pos_qid[key] = sum(pos_qid[key]) / len(pos_qid[key])\n",
    "    \n",
    "    for index in sorted(unwanted_index, reverse=True):\n",
    "        del data[index]\n",
    "    \n",
    "    for item in data:\n",
    "        item['avg_pos_uid'] = avg_pos_uid[item['uid']]\n",
    "        item['avg_pos_qid'] = avg_pos_qid[item['qid']]\n",
    "\n",
    "        \n",
    "def _feat_train(bd, sign_val=None, extra=None):\n",
    "    # Basic features\n",
    "    # qid(string), uid(string), position(float)\n",
    "    # answer'(string), 'potistion'(float), 'qid'(string), 'uid'(string)\n",
    "    X = _feat_basic(bd, group='train')\n",
    "    y = []\n",
    "    \n",
    "    # Some extra features\n",
    "    if extra:\n",
    "        for func_name in extra:\n",
    "            func_name = '_feat_' + func_name\n",
    "            if func_name in ['_feat_avg_pos']:\n",
    "                globals()[func_name](X, sign_val=sign_val)\n",
    "            else:\n",
    "                globals()[func_name](X)\n",
    "    \n",
    "    for item in X:\n",
    "        y.append(item['position'])\n",
    "        del item['position']\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature function(public)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize(bd, group='train', sign_val=None, extra=None):\n",
    "    if group == 'train':\n",
    "        return _feat_train(bd, sign_val=sign_val, extra=extra)\n",
    "    elif group == 'test':\n",
    "        return _feat_basic(bd, group='test')\n",
    "    else:\n",
    "        raise ValueError(group, 'is not the proper type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def select(data, keys):\n",
    "    unwanted = data[0].keys() - keys\n",
    "    for item in data:\n",
    "        for unwanted_key in unwanted:\n",
    "            del item[unwanted_key]\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_result(test_set, predictions, file_name='guess.csv'):\n",
    "    predictions = sorted([[id, predictions[index]] for index, id in enumerate(test_set.keys())])\n",
    "    predictions.insert(0,[\"id\", \"position\"])\n",
    "    with open(file_name, \"w\") as fp:\n",
    "        writer = csv.writer(fp, delimiter=',')\n",
    "        writer.writerows(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Linear Cross validation RMSE scores:\n",
      "LinearRegression 35.3976636587\n",
      "Ridge 85.4215957267\n",
      "Lasso 34.8350153225\n",
      "ElasticNet 46.015917262\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import math\n",
    "from numpy import abs, sqrt\n",
    "\n",
    "\n",
    "bd = load_buzz()\n",
    "X_train, y_train = featurize(bd, group='train', sign_val=None, extra=['sign_val', 'avg_pos'])\n",
    "regression_keys = ['category', 'q_length', 'sign_val', 'qid', 'uid', 'answer']\n",
    "X_train = select(X_train, regression_keys)\n",
    "X_test = featurize(bd, group='test')\n",
    "\n",
    "vec = DictVectorizer()\n",
    "X_train = vec.fit_transform(X_train)\n",
    "\n",
    "regressor_names = \"\"\"\n",
    "LinearRegression\n",
    "Ridge\n",
    "Lasso\n",
    "ElasticNet\n",
    "\"\"\"\n",
    "print (\"=== Linear Cross validation RMSE scores:\")\n",
    "for regressor in regressor_names.split():\n",
    "    scores = cross_val_score(getattr(linear_model, regressor)(),\n",
    "                             X_train, y_train,\n",
    "                             cv=10,\n",
    "                             scoring='mean_squared_error',\n",
    "                             n_jobs=multiprocessing.cpu_count()-1\n",
    "                            )\n",
    "    print (regressor, sqrt(abs(scores)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
       "    max_iter=1000, n_alphas=100, n_jobs=1, normalize=False, positive=False,\n",
       "    precompute='auto', random_state=None, selection='cyclic', tol=0.0001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd = load_buzz()\n",
    "X_train, y_train = featurize(bd, group='train', sign_val=None, extra=['sign_val'])\n",
    "regression_keys = ['category', 'q_length', 'sign_val', 'qid', 'uid', 'answer']\n",
    "X_train = select(X_train, regression_keys)\n",
    "X_test = featurize(bd, group='test')\n",
    "X_test = select(X_test, regression_keys)\n",
    "\n",
    "vec = DictVectorizer()\n",
    "vec.fit(X_train + X_test)\n",
    "X_train = vec.transform(X_train)\n",
    "X_test = vec.transform(X_test)\n",
    "                                                                      \n",
    "regressor = linear_model.LassoCV()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.00000000e+00   0.00000000e+00  -0.00000000e+00 ...,   5.00200833e-05\n",
      "   0.00000000e+00  -1.92318230e-02]\n",
      "146.866398809\n"
     ]
    }
   ],
   "source": [
    "print(regressor.coef_)\n",
    "print(regressor.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_result(bd['test'], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submissions scores 84.38453 in Kaggle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
